{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a272b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data impearly-risk-predictionort Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "os.makedirs(\"Features\", exist_ok=True)\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99245ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all json files and combine into a single csv file\n",
    "json_folder = \"final-eriskt2-dataset-with-ground-truth/all_combined\"\n",
    "label_file = \"final-eriskt2-dataset-with-ground-truth/shuffled_ground_truth_labels.txt\"\n",
    "output_file = \"Results/combined_dataset.csv\"\n",
    "\n",
    "\n",
    "labels = {}\n",
    "with open(label_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        user_id, label = line.strip().split()\n",
    "        labels[user_id] = 1 if label == \"1\" else 0\n",
    "\n",
    "\n",
    "header = [\n",
    "    \"user_id\", \"target\", \"type\", \"title\", \"body\",\n",
    "    \"created_utc\", \"submission_id\", \"parent_id\",\n",
    "    \"comment_id\", \"Depression\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "\n",
    "\n",
    "def safe_text(s):\n",
    "\n",
    "    if isinstance(s, str):\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            return None\n",
    "        try:\n",
    "\n",
    "            fixed = s.encode('latin1').decode('utf-8')\n",
    "            return fixed\n",
    "        except Exception:\n",
    "\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "# run all json files\n",
    "for filename in os.listdir(json_folder):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(json_folder, filename)\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "\n",
    "    for thread in data:\n",
    "        # ---------- submission ----------\n",
    "        sub = thread.get(\"submission\", {})\n",
    "        sub_user = sub.get(\"user_id\")\n",
    "        sub_target = sub.get(\"target\", False)\n",
    "        depression_label = labels.get(sub_user, None) if sub_target else None\n",
    "\n",
    "        rows.append({\n",
    "            \"user_id\": sub_user,\n",
    "            \"target\": sub_target,\n",
    "            \"type\": \"submission\",\n",
    "            \"title\": safe_text(sub.get(\"title\")),\n",
    "            \"body\": safe_text(sub.get(\"body\")),\n",
    "            \"created_utc\": sub.get(\"created_utc\"),\n",
    "            \"submission_id\": sub.get(\"submission_id\"),\n",
    "            \"parent_id\": sub.get(\"parent_id\"),\n",
    "            \"comment_id\": None,\n",
    "            \"Depression\": depression_label\n",
    "        })\n",
    "\n",
    "        # ---------- comments ----------\n",
    "        for c in thread.get(\"comments\", []):\n",
    "            c_user = c.get(\"user_id\")\n",
    "            c_target = c.get(\"target\", False)\n",
    "            depression_label = labels.get(c_user, None) if c_target else None\n",
    "\n",
    "            rows.append({\n",
    "                \"user_id\": c_user,\n",
    "                \"target\": c_target,\n",
    "                \"type\": \"comment\",\n",
    "                \"title\": None,\n",
    "                \"body\": safe_text(c.get(\"body\")),\n",
    "                \"created_utc\": c.get(\"created_utc\"),\n",
    "                \"submission_id\": c.get(\"submission_id\"),\n",
    "                \"parent_id\": c.get(\"parent_id\"),\n",
    "                \"comment_id\": c.get(\"comment_id\"),\n",
    "                \"Depression\": depression_label\n",
    "            })\n",
    "\n",
    "\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf179255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent text column and filter to target=True\n",
    "\n",
    "df = pd.read_csv(\"Results/combined_dataset.csv\")\n",
    "\n",
    "# build lookup dicts\n",
    "sub_dict = df[df['type']=='submission'].set_index('submission_id')['body'].to_dict()\n",
    "com_dict = df[df['type']=='comment'].set_index('comment_id')['body'].to_dict()\n",
    "\n",
    "def find_parent(row):\n",
    "    if row['type'] != 'comment':   \n",
    "        return None\n",
    "    \n",
    "    pid = row['parent_id']\n",
    "    if pd.isna(pid):\n",
    "        return None\n",
    "    \n",
    "    if pid in sub_dict:\n",
    "        return sub_dict[pid]\n",
    "    elif pid in com_dict:\n",
    "        return com_dict[pid]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['parent'] = df.apply(find_parent, axis=1)\n",
    "\n",
    "df= df[df[\"target\"] == True].copy()\n",
    "\n",
    "df.to_csv(\"Results/combined_dataset_with_parent.csv\", index=False)\n",
    "\n",
    "print(\"\\nNew file saved as: Results/combined_dataset_with_parent.csv\")\n",
    "print(\"Rows:\", len(df))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of samples for each class\n",
    "\n",
    "df = pd.read_csv(\"Results/combined_dataset_with_parent.csv\")\n",
    "\n",
    "count0 = (df[\"Depression\"] == 0).sum()\n",
    "count1 = (df[\"Depression\"] == 1).sum()\n",
    "\n",
    "print(\"Depression = 0  →\", count0)\n",
    "print(\"Depression = 1  →\", count1)\n",
    "print(\"\\nTotal labeled targets →\", count0 + count1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-level counts\n",
    "\n",
    "df = pd.read_csv(\"Results/combined_dataset_with_parent.csv\")\n",
    "\n",
    "df_labeled = df[df[\"Depression\"].notna()].copy()\n",
    "\n",
    "depressed_users = df_labeled[df_labeled[\"Depression\"] == 1][\"user_id\"].nunique()\n",
    "control_users   = df_labeled[df_labeled[\"Depression\"] == 0][\"user_id\"].nunique()\n",
    "total_users     = df_labeled[\"user_id\"].nunique()\n",
    "\n",
    "print(\" User-level counts:\")\n",
    "print(f\"Depression = 1 users : {depressed_users}\")\n",
    "print(f\"Depression = 0 users : {control_users}\")\n",
    "print(f\"Total labeled users  : {total_users}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081222ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split at user level\n",
    "\n",
    "df = pd.read_csv(\"Results/combined_dataset_with_parent.csv\")\n",
    "\n",
    "\n",
    "# all depression target users\n",
    "users_pos = df[df[\"Depression\"] == 1][\"user_id\"].unique()\n",
    "# all control target users\n",
    "users_neg = df[df[\"Depression\"] == 0][\"user_id\"].unique()\n",
    "\n",
    "# assign train/test split per user\n",
    "train_pos, test_pos = train_test_split(users_pos, test_size=0.2, random_state=42)\n",
    "train_neg, test_neg = train_test_split(users_neg, test_size=0.2, random_state=42)\n",
    "\n",
    "# put together\n",
    "train_users = set(train_pos) | set(train_neg)\n",
    "test_users  = set(test_pos)  | set(test_neg)\n",
    "\n",
    "# filter posts by user_id\n",
    "train_df = df[df[\"user_id\"].isin(train_users)].copy()\n",
    "test_df  = df[df[\"user_id\"].isin(test_users)].copy()\n",
    "\n",
    "# delete some Depression=0 item to reduce size and balance the dataset\n",
    "\n",
    "# depression=0 index\n",
    "train_zero_idx = train_df.index[train_df[\"Depression\"] == 0]\n",
    "print(\"Train: num of Dep=0\", len(train_zero_idx))\n",
    "\n",
    "test_zero_idx = test_df.index[test_df[\"Depression\"] == 0]\n",
    "print(\"Test: num of Dep=0\", len(test_zero_idx))\n",
    "# randomly select num to remove\n",
    "train_remove_idx = np.random.choice(train_zero_idx, size=200000, replace=False)\n",
    "# delete them\n",
    "train_df_filtered = train_df.drop(train_remove_idx).reset_index(drop=True)\n",
    "print(\"Train before:\", train_df.shape)\n",
    "print(\"Train after:\", train_df_filtered.shape)\n",
    "train_df=train_df_filtered\n",
    "\n",
    "test_remove_idx = np.random.choice(test_zero_idx, size=40000, replace=False)\n",
    "# delete them\n",
    "test_df_filtered = test_df.drop(test_remove_idx).reset_index(drop=True)\n",
    "print(\"Test before:\", test_df.shape)\n",
    "print(\"Test after:\", test_df_filtered.shape)\n",
    "test_df=test_df_filtered\n",
    "\n",
    "train_df.to_csv(\"Results/train_target_with_parent.csv\", index=False)\n",
    "test_df.to_csv(\"Results/test_target_with_parent.csv\", index=False)\n",
    "\n",
    "print(\" Train - Test Split\")\n",
    "print(f\"Train users: {train_df['user_id'].nunique()} | Posts: {len(train_df)}\")\n",
    "print(f\"Test  users: {test_df['user_id'].nunique()} | Posts: {len(test_df)}\")\n",
    "print(f\"Dep1-train={sum(train_df.Depression==1)}, Dep1-test={sum(test_df.Depression==1)}\")\n",
    "print(f\"Dep0-train={sum(train_df.Depression==0)}, Dep0-test={sum(test_df.Depression==0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LIWC features\n",
    "!export NLTK_DATA=\"/nfs/u50/zhanh279/4Z03/jupyter/nltk_data\"\n",
    "!python liwc_script.py  --data Results/train_target_with_parent.csv --column body --save train_LIWC_target\n",
    "!python liwc_script.py  --data Results/train_target_with_parent.csv --column parent --save train_LIWC_parent\n",
    "!python liwc_script.py  --data Results/test_target_with_parent.csv  --column body --save test_LIWC_target\n",
    "!python liwc_script.py  --data Results/test_target_with_parent.csv  --column parent --save test_LIWC_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LSM features based on LIWC features for train and test\n",
    "\n",
    "############################## Training set ##############################\n",
    "# Load data\n",
    "body_df = pd.read_pickle(\"/u50/zhanh279/4Z03/jupyter/Results/train_LIWC_target.fullframe.pickle\")\n",
    "parent_df = pd.read_pickle(\"/u50/zhanh279/4Z03/jupyter/Results/train_LIWC_parent.fullframe.pickle\")\n",
    "\n",
    "\n",
    "body_df = body_df[body_df['type'] == 'comment'].reset_index(drop=True)\n",
    "parent_df = parent_df[parent_df['type'] == 'comment'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# define LIWC columns\n",
    "\n",
    "liwc_cols = [\"ARTICLE\", \"AUXVERB\", \"CONJ\", \"ADVERB\", \"PPRON\", \"IPRON\", \"PREP\", \"NEGATE\", \"QUANT\", \"VERB\"]\n",
    "social_cols = [\"SOCIAL\",\"FRIEND\",\"FAMILY\",\"WE\",\"YOU\",\"THEY\",\"AFFILIATION\",\"DRIVES\",\"POWER\",\"RELIG\",\"HEALTH\",\"WORK\",\"MONEY\"]\n",
    "positive_emo_cols = [\"POSEMO\",\"REWARD\",\"ACHIEV\",\"JOY\",\"AFFILIATION\",\"CERTAIN\",\"POWER\"]\n",
    "negative_emo_cols = [\"NEGEMO\",\"SAD\",\"ANX\",\"ANGER\",\"RISK\",\"DEATH\",\"FEAR\",\"DISCREP\",\"TENTAT\"]\n",
    "first_person_singular_cols = [\"I\"]\n",
    "second_person_cols = [\"YOU\"]\n",
    "third_person_singular_cols = [\"SHEHE\"]\n",
    "third_person_plural_cols = [\"THEY\"]   \n",
    "cognitive_process_cols = [\"CAUSE\",\"DISCREP\",\"INSIGHT\",\"CERTAIN\",\"COGPROC\"]\n",
    "perceptual_process_cols = [\"SEE\",\"HEAR\",\"FEEL\",\"PERCEPT\"]\n",
    "\n",
    "# all needed LIWC numerical columns\n",
    "all_needed_cols = sorted(list(set(\n",
    "      liwc_cols\n",
    "    + social_cols\n",
    "    + positive_emo_cols\n",
    "    + negative_emo_cols\n",
    "    + first_person_singular_cols\n",
    "    + second_person_cols\n",
    "    + third_person_singular_cols\n",
    "    + third_person_plural_cols\n",
    "    + cognitive_process_cols\n",
    "    + perceptual_process_cols\n",
    ")))\n",
    "\n",
    "# filter dfs early\n",
    "needed_cols = [\"comment_id\"] + all_needed_cols + [\"user_id\",\"body\",\"type\",\"Depression\",\"created_utc\",\"submission_id\"]\n",
    "\n",
    "body_df = body_df[needed_cols]\n",
    "parent_df = parent_df[needed_cols]\n",
    "\n",
    "# compute LSM\n",
    "EPS = 1e-6\n",
    "LSM_df = pd.DataFrame()\n",
    "\n",
    "for c in all_needed_cols:\n",
    "    LSM_df[f\"LSM_{c}\"] = 1 - abs(body_df[c] - parent_df[c]) / (\n",
    "        body_df[c] + parent_df[c] + EPS\n",
    "    )\n",
    "\n",
    "# group means\n",
    "LSM_df[\"LSM_mean\"] = LSM_df[[f\"LSM_{c}\" for c in liwc_cols]].mean(axis=1)\n",
    "LSM_df[\"social_mean\"] = LSM_df[[f\"LSM_{c}\" for c in social_cols]].mean(axis=1)\n",
    "LSM_df[\"positive_mean\"] = LSM_df[[f\"LSM_{c}\" for c in positive_emo_cols]].mean(axis=1)\n",
    "LSM_df[\"negative_mean\"] = LSM_df[[f\"LSM_{c}\" for c in negative_emo_cols]].mean(axis=1)\n",
    "LSM_df[\"first_person_singular\"] = LSM_df[[f\"LSM_{c}\" for c in first_person_singular_cols]].mean(axis=1)\n",
    "LSM_df[\"second_person\"] = LSM_df[[f\"LSM_{c}\" for c in second_person_cols]].mean(axis=1)\n",
    "LSM_df[\"third_person_singular\"] = LSM_df[[f\"LSM_{c}\" for c in third_person_singular_cols]].mean(axis=1)\n",
    "LSM_df[\"third_person_plural\"] = LSM_df[[f\"LSM_{c}\" for c in third_person_plural_cols]].mean(axis=1)\n",
    "LSM_df[\"cognitive_process\"] = LSM_df[[f\"LSM_{c}\" for c in cognitive_process_cols]].mean(axis=1)\n",
    "LSM_df[\"perceptual_process\"] = LSM_df[[f\"LSM_{c}\" for c in perceptual_process_cols]].mean(axis=1)\n",
    "\n",
    "# final output\n",
    "result_df = pd.concat([\n",
    "    body_df[[\"user_id\",\"body\",\"type\",\"Depression\",\"created_utc\",\"comment_id\",\"submission_id\"]],\n",
    "    parent_df[\"body\"].rename(\"parent_body\"),\n",
    "    LSM_df[\n",
    "        [\"LSM_mean\",\"social_mean\",\"positive_mean\",\"negative_mean\",\n",
    "         \"first_person_singular\",\"second_person\",\"third_person_singular\",\n",
    "         \"third_person_plural\",\"cognitive_process\",\"perceptual_process\"]\n",
    "    ]\n",
    "], axis=1)\n",
    "count_submission = (result_df[\"type\"] == \"submission\").sum()\n",
    "print(\"Number of submissions in training LSM features:\", count_submission)\n",
    "result_df.to_pickle(\"Results/train_LSM_features.pickle\")\n",
    "print(\"Train Saved:\", len(result_df))\n",
    "\n",
    "\n",
    "############################## Testing set ##############################\n",
    "# Load data\n",
    "body_df = pd.read_pickle(\"/u50/zhanh279/4Z03/jupyter/Results/test_LIWC_target.fullframe.pickle\")\n",
    "parent_df = pd.read_pickle(\"/u50/zhanh279/4Z03/jupyter/Results/test_LIWC_parent.fullframe.pickle\")\n",
    "body_df = body_df[body_df['type'] == 'comment']\n",
    "parent_df = parent_df[parent_df['type'] == 'comment']\n",
    "# define LIWC columns\n",
    "\n",
    "liwc_cols = [\"ARTICLE\", \"AUXVERB\", \"CONJ\", \"ADVERB\", \"PPRON\", \"IPRON\", \"PREP\", \"NEGATE\", \"QUANT\", \"VERB\"]\n",
    "social_cols = [\"SOCIAL\",\"FRIEND\",\"FAMILY\",\"WE\",\"YOU\",\"THEY\",\"AFFILIATION\",\"DRIVES\",\"POWER\",\"RELIG\",\"HEALTH\",\"WORK\",\"MONEY\"]\n",
    "positive_emo_cols = [\"POSEMO\",\"REWARD\",\"ACHIEV\",\"JOY\",\"AFFILIATION\",\"CERTAIN\",\"POWER\"]\n",
    "negative_emo_cols = [\"NEGEMO\",\"SAD\",\"ANX\",\"ANGER\",\"RISK\",\"DEATH\",\"FEAR\",\"DISCREP\",\"TENTAT\"]\n",
    "first_person_singular_cols = [\"I\"]\n",
    "second_person_cols = [\"YOU\"]\n",
    "third_person_singular_cols = [\"SHEHE\"]\n",
    "third_person_plural_cols = [\"THEY\"]   \n",
    "cognitive_process_cols = [\"CAUSE\",\"DISCREP\",\"INSIGHT\",\"CERTAIN\",\"COGPROC\"]\n",
    "perceptual_process_cols = [\"SEE\",\"HEAR\",\"FEEL\",\"PERCEPT\"]\n",
    "\n",
    "# all needed LIWC numerical columns\n",
    "all_needed_cols = sorted(list(set(\n",
    "      liwc_cols\n",
    "    + social_cols\n",
    "    + positive_emo_cols\n",
    "    + negative_emo_cols\n",
    "    + first_person_singular_cols\n",
    "    + second_person_cols\n",
    "    + third_person_singular_cols\n",
    "    + third_person_plural_cols\n",
    "    + cognitive_process_cols\n",
    "    + perceptual_process_cols\n",
    ")))\n",
    "\n",
    "# filter dfs early\n",
    "needed_cols = [\"comment_id\"] + all_needed_cols + [\"user_id\",\"body\",\"type\",\"Depression\",\"created_utc\",\"submission_id\"]\n",
    "\n",
    "body_df = body_df[needed_cols]\n",
    "parent_df = parent_df[needed_cols]\n",
    "\n",
    "# compute LSM\n",
    "EPS = 1e-6\n",
    "LSM_df = pd.DataFrame()\n",
    "\n",
    "for c in all_needed_cols:\n",
    "    LSM_df[f\"LSM_{c}\"] = 1 - abs(body_df[c] - parent_df[c]) / (\n",
    "        body_df[c] + parent_df[c] + EPS\n",
    "    )\n",
    "\n",
    "# group means\n",
    "LSM_df[\"LSM_mean\"] = LSM_df[[f\"LSM_{c}\" for c in liwc_cols]].mean(axis=1)\n",
    "LSM_df[\"social_mean\"] = LSM_df[[f\"LSM_{c}\" for c in social_cols]].mean(axis=1)\n",
    "LSM_df[\"positive_mean\"] = LSM_df[[f\"LSM_{c}\" for c in positive_emo_cols]].mean(axis=1)\n",
    "LSM_df[\"negative_mean\"] = LSM_df[[f\"LSM_{c}\" for c in negative_emo_cols]].mean(axis=1)\n",
    "LSM_df[\"first_person_singular\"] = LSM_df[[f\"LSM_{c}\" for c in first_person_singular_cols]].mean(axis=1)\n",
    "LSM_df[\"second_person\"] = LSM_df[[f\"LSM_{c}\" for c in second_person_cols]].mean(axis=1)\n",
    "LSM_df[\"third_person_singular\"] = LSM_df[[f\"LSM_{c}\" for c in third_person_singular_cols]].mean(axis=1)\n",
    "LSM_df[\"third_person_plural\"] = LSM_df[[f\"LSM_{c}\" for c in third_person_plural_cols]].mean(axis=1)\n",
    "LSM_df[\"cognitive_process\"] = LSM_df[[f\"LSM_{c}\" for c in cognitive_process_cols]].mean(axis=1)\n",
    "LSM_df[\"perceptual_process\"] = LSM_df[[f\"LSM_{c}\" for c in perceptual_process_cols]].mean(axis=1)\n",
    "\n",
    "# final output\n",
    "result_df = pd.concat([\n",
    "    body_df[[\"user_id\",\"body\",\"type\",\"Depression\",\"created_utc\",\"comment_id\",\"submission_id\"]],\n",
    "    parent_df[\"body\"].rename(\"parent_body\"),\n",
    "    LSM_df[\n",
    "        [\"LSM_mean\",\"social_mean\",\"positive_mean\",\"negative_mean\",\n",
    "         \"first_person_singular\",\"second_person\",\"third_person_singular\",\n",
    "         \"third_person_plural\",\"cognitive_process\",\"perceptual_process\"]\n",
    "    ]\n",
    "], axis=1)\n",
    "result_df = result_df[result_df['type'] == 'comment']\n",
    "result_df.to_pickle(\"Results/test_LSM_features.pickle\")\n",
    "print(\"Test Saved:\", len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72577c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features for LSM\n",
    "\n",
    "lsm_train=pd.read_pickle(\"Results/train_LSM_features.pickle\")\n",
    "lsm_test=pd.read_pickle(\"Results/test_LSM_features.pickle\")\n",
    "\n",
    "# add features\n",
    "lsm_features = [\n",
    "    'LSM_mean', 'social_mean', 'positive_mean', 'negative_mean',\n",
    "    'first_person_singular', 'second_person',\n",
    "    'third_person_singular', 'third_person_plural',\n",
    "    'cognitive_process', 'perceptual_process'\n",
    "]\n",
    "def add_timeseries_features(df, feature_cols, window=5):\n",
    "\n",
    "    for col in feature_cols:\n",
    "\n",
    "        # max so far\n",
    "        df[f\"{col}_max_so_far\"] = df.groupby(\"user_id\")[col].cummax()\n",
    "\n",
    "        # min so far\n",
    "        df[f\"{col}_min_so_far\"] = df.groupby(\"user_id\")[col].cummin()\n",
    "\n",
    "        # max gap so far\n",
    "        df[f\"{col}_max_gap_so_far\"] = (\n",
    "            df[f\"{col}_max_so_far\"] - df[f\"{col}_min_so_far\"]\n",
    "        )\n",
    "\n",
    "        # delta to previous post\n",
    "        df[f\"{col}_delta\"] = df.groupby(\"user_id\")[col].diff().fillna(0)\n",
    "\n",
    "        # rolling standard deviation\n",
    "        df[f\"{col}_rolling_std\"] = (\n",
    "            df.groupby(\"user_id\")[col]\n",
    "              .rolling(window=window, min_periods=1)\n",
    "              .std()\n",
    "              .reset_index(level=0, drop=True)\n",
    "              .fillna(0)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "lsm_train = add_timeseries_features(lsm_train, lsm_features, window=5)\n",
    "lsm_test  = add_timeseries_features(lsm_test, lsm_features, window=5)\n",
    "\n",
    "lsm_train.to_pickle(\"Features/LSM_features_train.pickle\")\n",
    "lsm_test.to_pickle(\"Features/LSM_features_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee79199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate relative entropy features \n",
    "\n",
    "# model definition and finetuning code\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.tok = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
    "        return enc\n",
    "\n",
    "def finetune_lm(texts, save_path, epochs=3, batch_size=8, lr=5e-5):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "  \n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    ds = TextDataset(texts, tokenizer)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dl:\n",
    "            batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {ep+1} loss = {total_loss/len(dl):.4f}\")\n",
    "\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "#get RE\n",
    "def load_lm(path):\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(path).to(DEVICE)\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "def calc_loss(model, tok, text):\n",
    "    enc = tok(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    enc = {k: v.to(DEVICE) for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc, labels=enc[\"input_ids\"])\n",
    "    # divide by number of tokens\n",
    "    return out.loss.item() / enc[\"input_ids\"].size(1)\n",
    "\n",
    "\n",
    "# Finetune language models on training data\n",
    "train_all_target=pd.read_csv(\"Results/train_target_with_parent.csv\")\n",
    "train_all_target=train_all_target.dropna(subset=['body'])\n",
    "train_dep0=train_all_target[train_all_target[\"Depression\"]==0]\n",
    "train_dep1=train_all_target[train_all_target[\"Depression\"]==1]\n",
    "\n",
    "train_dep0_texts = train_dep0[\"body\"].astype(str).tolist()\n",
    "train_dep1_texts = train_dep1[\"body\"].astype(str).tolist()\n",
    "\n",
    "finetune_lm(train_dep0_texts, \"models/train_Dep0_LM\", epochs=3)\n",
    "finetune_lm(train_dep1_texts, \"models/train_Dep1_LM\", epochs=3)\n",
    "\n",
    "tok0, lm0 = load_lm(\"models/train_Dep0_LM\")\n",
    "tok1, lm1 = load_lm(\"models/train_Dep1_LM\")\n",
    "\n",
    "# calculate RE features for train sets\n",
    "train_all_target[\"body\"] = train_all_target[\"body\"].astype(str)\n",
    "\n",
    "\n",
    "loss_dep0 = []\n",
    "loss_dep1 = []\n",
    "\n",
    "for text in train_all_target[\"body\"]:\n",
    "    loss_dep0.append(calc_loss(lm0, tok0, text))\n",
    "    loss_dep1.append(calc_loss(lm1, tok1, text))\n",
    "\n",
    "train_all_target[\"loss_dep0\"] = loss_dep0\n",
    "train_all_target[\"loss_dep1\"] = loss_dep1\n",
    "train_all_target[\"re\"] = train_all_target[\"loss_dep1\"] - train_all_target[\"loss_dep0\"]\n",
    "\n",
    "print(train_all_target.columns)\n",
    "\n",
    "# calculate RE features for test sets\n",
    "test_all_target=pd.read_csv(\"Results/test_target_with_parent.csv\")\n",
    "test_all_target=test_all_target.dropna(subset=['body'])\n",
    "test_all_target[\"body\"] = test_all_target[\"body\"].astype(str)\n",
    "loss_dep0 = []\n",
    "loss_dep1 = []\n",
    "for text in test_all_target[\"body\"]:\n",
    "    loss_dep0.append(calc_loss(lm0, tok0, text))\n",
    "    loss_dep1.append(calc_loss(lm1, tok1, text))\n",
    "test_all_target[\"loss_dep0\"] = loss_dep0\n",
    "test_all_target[\"loss_dep1\"] = loss_dep1\n",
    "test_all_target[\"re\"] = test_all_target[\"loss_dep1\"] - test_all_target[\"loss_dep0\"]\n",
    "\n",
    "print(test_all_target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adding features to RE\n",
    "# maximum value so far\n",
    "train_all_target[\"max_re_so_far\"] = train_all_target.groupby(\"user_id\")[\"re\"].cummax()\n",
    "\n",
    "# minimum value so far\n",
    "train_all_target[\"min_re_so_far\"] = train_all_target.groupby(\"user_id\")[\"re\"].cummin()\n",
    "\n",
    "# maximum gap so far\n",
    "train_all_target[\"max_gap_re_so_far\"] = train_all_target[\"max_re_so_far\"] - train_all_target[\"min_re_so_far\"]\n",
    "\n",
    "# current-last\n",
    "train_all_target[\"delta_re\"] = train_all_target.groupby(\"user_id\")[\"re\"].diff().fillna(0)\n",
    "\n",
    "# rolling std\n",
    "train_all_target[\"rolling_std_re\"] = (\n",
    "    train_all_target.groupby(\"user_id\")[\"re\"]\n",
    "         .rolling(window=5, min_periods=1)\n",
    "         .std()\n",
    "         .reset_index(level=0, drop=True)\n",
    "         .fillna(0)\n",
    ")\n",
    "\n",
    "# maximum value so far\n",
    "test_all_target[\"max_re_so_far\"] = test_all_target.groupby(\"user_id\")[\"re\"].cummax()\n",
    "\n",
    "# minimum value so far\n",
    "test_all_target[\"min_re_so_far\"] = test_all_target.groupby(\"user_id\")[\"re\"].cummin()\n",
    "\n",
    "# maximum gap so far\n",
    "test_all_target[\"max_gap_re_so_far\"] = test_all_target[\"max_re_so_far\"] - test_all_target[\"min_re_so_far\"]\n",
    "\n",
    "# current-last\n",
    "test_all_target[\"delta_re\"] = test_all_target.groupby(\"user_id\")[\"re\"].diff().fillna(0)\n",
    "\n",
    "# rolling std\n",
    "test_all_target[\"rolling_std_re\"] = (\n",
    "    test_all_target.groupby(\"user_id\")[\"re\"]\n",
    "         .rolling(window=5, min_periods=1)\n",
    "         .std()\n",
    "         .reset_index(level=0, drop=True)\n",
    "         .fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "print(train_all_target.columns)\n",
    "train_all_target.to_pickle(\"Features/RE_features_train.pickle\")\n",
    "test_all_target.to_pickle(\"Features/RE_features_test.pickle\")\n",
    "\n",
    "print(len(train_all_target))\n",
    "print(len(test_all_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd31594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Cosine features\n",
    "\n",
    "# Load data\n",
    "cos_train=pd.read_csv(\"Results/train_target_with_parent.csv\")\n",
    "cos_test=pd.read_csv(\"Results/test_target_with_parent.csv\")\n",
    "cos_train = cos_train[cos_train['type']=='comment'].reset_index(drop=True)\n",
    "cos_test  = cos_test[cos_test['type']=='comment'].reset_index(drop=True)\n",
    "\n",
    "# clean NAN\n",
    "cos_train[\"body\"] = cos_train[\"body\"].fillna(\"\").astype(str)\n",
    "cos_train[\"parent\"] = cos_train[\"parent\"].fillna(\"\").astype(str)\n",
    "\n",
    "cos_test[\"body\"] = cos_test[\"body\"].fillna(\"\").astype(str)\n",
    "cos_test[\"parent\"] = cos_test[\"parent\"].fillna(\"\").astype(str)\n",
    "\n",
    "print(cos_train.columns)\n",
    "\n",
    "\n",
    "# Load SentenceTransformer\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=DEVICE)\n",
    "# Encode & Compute Cosine Similarity\n",
    "def compute_cos_sim_batch(df, batch_size=256):\n",
    "    sims = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        texts_t = df[\"body\"].iloc[i:i+batch_size].tolist()\n",
    "        texts_p = df[\"parent\"].iloc[i:i+batch_size].tolist()\n",
    "\n",
    "        emb_t = encoder.encode(texts_t, batch_size=32, convert_to_tensor=True)\n",
    "        emb_p = encoder.encode(texts_p, batch_size=32, convert_to_tensor=True)\n",
    "\n",
    "        # compute pairwise but only diagonal\n",
    "        batch_sims = util.cos_sim(emb_t, emb_p).diagonal().cpu().numpy()\n",
    "        sims.extend(batch_sims)\n",
    "\n",
    "    return np.array(sims)\n",
    "\n",
    "print(\"Encoding train...\")\n",
    "cos_train[\"sim\"] = compute_cos_sim_batch(cos_train)\n",
    "\n",
    "print(\"Encoding test...\")\n",
    "cos_test[\"sim\"] = compute_cos_sim_batch(cos_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0499ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features for Cosine Similarity\n",
    "def add_sim_features(df):\n",
    "    df[\"max_sim_so_far\"] = df.groupby(\"user_id\")[\"sim\"].cummax()\n",
    "    df[\"min_sim_so_far\"] = df.groupby(\"user_id\")[\"sim\"].cummin()\n",
    "    df[\"max_gap_sim_so_far\"] = df[\"max_sim_so_far\"] - df[\"min_sim_so_far\"]\n",
    "    df[\"delta_sim\"] = df.groupby(\"user_id\")[\"sim\"].diff().fillna(0)\n",
    "    df[\"rolling_std_sim\"] = (\n",
    "        df.groupby(\"user_id\")[\"sim\"]\n",
    "          .rolling(window=5, min_periods=1)\n",
    "          .std()\n",
    "          .reset_index(level=0, drop=True)\n",
    "          .fillna(0)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "cos_train = add_sim_features(cos_train)\n",
    "cos_test  = add_sim_features(cos_test)\n",
    "\n",
    "cos_train.to_pickle(\"Features/cos_sim_features_train.pickle\")\n",
    "cos_test.to_pickle(\"Features/cos_sim_features_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaget PHQ9 features\n",
    "!cd extremism\n",
    "!python item-scoring/item_scoring.py   --custom-dataset ../Results/train_target_with_parent.csv --text-column body --scale PHQ-9_archetype_scale --device cuda\n",
    "!python item-scoring/item_scoring.py   --custom-dataset ../Results/test_target_with_parent.csv --text-column body --scale PHQ-9_archetype_scale --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8e74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133430\n"
     ]
    }
   ],
   "source": [
    "# put features and user info together\n",
    "phq9_train=pd.read_pickle(\"Results/train_target_with_parent_PHQ-9_archetype_scale_sim.pickle\")\n",
    "user_info_train=pd.read_csv(\"Results/train_target_with_parent.csv\")\n",
    "phq9_train=pd.concat([user_info_train, phq9_train], axis=1)\n",
    "\n",
    "phq9_test=pd.read_pickle(\"Results/test_target_with_parent_PHQ-9_archetype_scale_sim.pickle\")\n",
    "user_info_test=pd.read_csv(\"Results/test_target_with_parent.csv\")\n",
    "phq9_test=pd.concat([user_info_test, phq9_test], axis=1)\n",
    "\n",
    "print(len(phq9_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'target', 'type', 'title', 'body', 'created_utc',\n",
      "       'submission_id', 'parent_id', 'comment_id', 'Depression', 'parent',\n",
      "       'PHQ-9_archetype_scale.0.sim', 'PHQ-9_archetype_scale.1.sim',\n",
      "       'PHQ-9_archetype_scale.2.sim', 'PHQ-9_archetype_scale.3.sim',\n",
      "       'PHQ-9_archetype_scale.4.sim', 'PHQ-9_archetype_scale.5.sim',\n",
      "       'PHQ-9_archetype_scale.6.sim', 'PHQ-9_archetype_scale.7.sim',\n",
      "       'PHQ-9_archetype_scale.8.sim', 'phq_score', 'max_phq_so_far',\n",
      "       'min_phq_so_far', 'max_gap', 'delta_phq', 'rolling_std_phq',\n",
      "       'post_index', 'max_0_so_far', 'max_1_so_far', 'max_2_so_far',\n",
      "       'max_3_so_far', 'max_4_so_far', 'max_5_so_far', 'max_6_so_far',\n",
      "       'max_7_so_far', 'max_8_so_far'],\n",
      "      dtype='object')\n",
      "Index(['user_id', 'target', 'type', 'title', 'body', 'created_utc',\n",
      "       'submission_id', 'parent_id', 'comment_id', 'Depression', 'parent',\n",
      "       'PHQ-9_archetype_scale.0.sim', 'PHQ-9_archetype_scale.1.sim',\n",
      "       'PHQ-9_archetype_scale.2.sim', 'PHQ-9_archetype_scale.3.sim',\n",
      "       'PHQ-9_archetype_scale.4.sim', 'PHQ-9_archetype_scale.5.sim',\n",
      "       'PHQ-9_archetype_scale.6.sim', 'PHQ-9_archetype_scale.7.sim',\n",
      "       'PHQ-9_archetype_scale.8.sim', 'phq_score', 'max_phq_so_far',\n",
      "       'min_phq_so_far', 'max_gap', 'delta_phq', 'rolling_std_phq',\n",
      "       'post_index', 'max_0_so_far', 'max_1_so_far', 'max_2_so_far',\n",
      "       'max_3_so_far', 'max_4_so_far', 'max_5_so_far', 'max_6_so_far',\n",
      "       'max_7_so_far', 'max_8_so_far'],\n",
      "      dtype='object')\n",
      "133430\n",
      "45762\n",
      "133430\n",
      "title                          104231\n",
      "parent                          50604\n",
      "parent_id                       29199\n",
      "comment_id                      29199\n",
      "body                            17951\n",
      "type                                0\n",
      "user_id                             0\n",
      "target                              0\n",
      "submission_id                       0\n",
      "created_utc                         0\n",
      "Depression                          0\n",
      "PHQ-9_archetype_scale.0.sim         0\n",
      "PHQ-9_archetype_scale.1.sim         0\n",
      "PHQ-9_archetype_scale.2.sim         0\n",
      "PHQ-9_archetype_scale.3.sim         0\n",
      "PHQ-9_archetype_scale.4.sim         0\n",
      "PHQ-9_archetype_scale.5.sim         0\n",
      "PHQ-9_archetype_scale.6.sim         0\n",
      "PHQ-9_archetype_scale.7.sim         0\n",
      "PHQ-9_archetype_scale.8.sim         0\n",
      "phq_score                           0\n",
      "max_phq_so_far                      0\n",
      "min_phq_so_far                      0\n",
      "max_gap                             0\n",
      "delta_phq                           0\n",
      "rolling_std_phq                     0\n",
      "post_index                          0\n",
      "max_0_so_far                        0\n",
      "max_1_so_far                        0\n",
      "max_2_so_far                        0\n",
      "max_3_so_far                        0\n",
      "max_4_so_far                        0\n",
      "max_5_so_far                        0\n",
      "max_6_so_far                        0\n",
      "max_7_so_far                        0\n",
      "max_8_so_far                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add more features for PHQ9\n",
    "\n",
    "phq_cols = [\n",
    "    'PHQ-9_archetype_scale.0.sim', 'PHQ-9_archetype_scale.1.sim',\n",
    "    'PHQ-9_archetype_scale.2.sim', 'PHQ-9_archetype_scale.3.sim',\n",
    "    'PHQ-9_archetype_scale.4.sim', 'PHQ-9_archetype_scale.5.sim',\n",
    "    'PHQ-9_archetype_scale.6.sim', 'PHQ-9_archetype_scale.7.sim',\n",
    "    'PHQ-9_archetype_scale.8.sim'\n",
    "]\n",
    "\n",
    "# Adding new features for train set\n",
    "phq9_train[\"phq_score\"] = phq9_train[phq_cols].mean(axis=1)\n",
    "phq9_train[\"max_phq_so_far\"] = phq9_train.groupby(\"user_id\")[\"phq_score\"].cummax()\n",
    "phq9_train[\"min_phq_so_far\"] = phq9_train.groupby(\"user_id\")[\"phq_score\"].cummin()\n",
    "phq9_train[\"max_gap\"]=phq9_train[\"max_phq_so_far\"]-phq9_train[\"min_phq_so_far\"] \n",
    "phq9_train[\"delta_phq\"] = phq9_train.groupby(\"user_id\")[\"phq_score\"].diff()\n",
    "phq9_train[\"rolling_std_phq\"] = (\n",
    "    phq9_train.groupby(\"user_id\")[\"phq_score\"].rolling(window=5, min_periods=1).std().reset_index(level=0, drop=True)\n",
    ")\n",
    "phq9_train[\"delta_phq\"] = phq9_train[\"delta_phq\"].fillna(0)\n",
    "phq9_train[\"rolling_std_phq\"] = phq9_train[\"rolling_std_phq\"].fillna(0)\n",
    "phq9_train[\"post_index\"] = phq9_train.groupby(\"user_id\").cumcount()\n",
    "for col in phq_cols:\n",
    "    scale_idx = col.split(\".\")[1].replace(\"sim\",\"\")  # extract \"scaleX\"\n",
    "    new_col = f\"max_{scale_idx}_so_far\"\n",
    "    phq9_train[new_col] = phq9_train.groupby(\"user_id\")[col].cummax()\n",
    "\n",
    "# Adding new features for test set\n",
    "phq9_test[\"phq_score\"] = phq9_test[phq_cols].mean(axis=1)\n",
    "phq9_test[\"max_phq_so_far\"] = phq9_test.groupby(\"user_id\")[\"phq_score\"].cummax()\n",
    "phq9_test[\"min_phq_so_far\"] = phq9_test.groupby(\"user_id\")[\"phq_score\"].cummin()\n",
    "phq9_test[\"max_gap\"]=phq9_test[\"max_phq_so_far\"]-phq9_test[\"min_phq_so_far\"] \n",
    "phq9_test[\"delta_phq\"] = phq9_test.groupby(\"user_id\")[\"phq_score\"].diff()\n",
    "phq9_test[\"rolling_std_phq\"] = (\n",
    "    phq9_test.groupby(\"user_id\")[\"phq_score\"].rolling(window=5, min_periods=1).std().reset_index(level=0, drop=True)\n",
    ")\n",
    "phq9_test[\"delta_phq\"] = phq9_test[\"delta_phq\"].fillna(0)\n",
    "phq9_test[\"rolling_std_phq\"] = phq9_test[\"rolling_std_phq\"].fillna(0)\n",
    "phq9_test[\"post_index\"] = phq9_test.groupby(\"user_id\").cumcount()\n",
    "for col in phq_cols:\n",
    "    scale_idx = col.split(\".\")[1].replace(\"sim\",\"\")  # extract \"scaleX\"\n",
    "    new_col = f\"max_{scale_idx}_so_far\"\n",
    "    phq9_test[new_col] = phq9_test.groupby(\"user_id\")[col].cummax()\n",
    "\n",
    "print(phq9_train.columns)\n",
    "print(phq9_test.columns)\n",
    "print(len(phq9_train))\n",
    "print(len(phq9_test))\n",
    "phq9_train.to_pickle(\"/u50/zhanh279/4Z03/jupyter/Features/PHQ9_features_train.pickle\")\n",
    "phq9_test.to_pickle(\"/u50/zhanh279/4Z03/jupyter/Features/PHQ9_features_test.pickle\")\n",
    "\n",
    "print(len(phq9_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b83c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "hongyi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
