
import requests
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import os
import sys
import time
from typing import Dict, List, Tuple
from datetime import datetime
import logging
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModel

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# æ·»åŠ ç›¸å…³æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.insert(0, os.path.join(parent_dir, 'extremism', 'item-scoring'))

# ä»configå¯¼å…¥æ‰€æœ‰é…ç½®
from config import (
    TEAM_TOKEN, NUM_RUNS, DECISION_THRESHOLD, 
    MAX_RETRIES, RETRY_DELAY, REQUEST_TIMEOUT, ROUND_INTERVAL,
    ACTIVE_SERVER, MODEL_PATH, MODEL_CONFIG, FEATURE_COLS,
    RUN_STRATEGIES
)

# ============================================================================
# é…ç½®æ—¥å¿—
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('erisk_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥LIWCæ¨¡å—
try:
    from liwc import extract_liwc, liwc_keys
except ImportError:
    logger.warning("LIWC module not found, using fallback implementation")
    liwc_keys = ['ACHIEV', 'ADJ', 'ADVERB', 'AFFECT', 'AFFILIATION', 'ANGER', 'ANX', 'ARTICLE', 
                'AUXVERB', 'BIO', 'BODY', 'CAUSE', 'CERTAIN', 'COGMECH', 'COMPARE', 'CONJ', 
                'DEATH', 'DIFFER', 'DISCREP', 'EXCL', 'FAMILY', 'FEEL', 'FEMALE', 'FILLER', 
                'FRIEND', 'FUTURE', 'HEALTH', 'HEAR', 'HOME', 'HUMANS', 'I', 'INCL', 'INHIBIT', 
                'INSIGHT', 'JOB', 'LEISURE', 'MALE', 'MONEY', 'MOTION', 'NEGATE', 'NONFLU', 
                'NUMBER', 'OTHER', 'OVER', 'PAST', 'PERCEPT', 'POSEMO', 'POSFEEL', 'PRESENT', 
                'PREPS', 'PRONOUN', 'QUANT', 'RELATIV', 'RELIG', 'SAD', 'SCHOOL', 'SEE', 'SELF', 
                'SEXUAL', 'SHEHE', 'SLEEP', 'SOCIAL', 'SPACE', 'SWEAR', 'TENTAT', 'TIME', 
                'TV', 'UP', 'WE', 'WORK', 'YOU', 'NEGEMO']
    
    def extract_liwc(text):
        # Fallback implementation returns zeros
        return {key: 0.0 for key in liwc_keys}

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('erisk_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# ä½¿ç”¨configä¸­çš„é…ç½®
# ============================================================================

BASE_URL = ACTIVE_SERVER


# ============================================================================
# Transformeræ¨¡å‹å®šä¹‰ï¼ˆæ¥è‡ªFinal_modelï¼‰
# ============================================================================

class TransformerUserClassifier(nn.Module):
    """
    ç”¨æˆ·çº§åˆ«çš„Transformeråˆ†ç±»å™¨
    å¤„ç†å˜é•¿çš„ç”¨æˆ·æ¶ˆæ¯åºåˆ—
    """
    def __init__(self, input_dim, hidden_dim=None, n_heads=None, n_layers=None, dropout=None):
        super().__init__()
        # å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨configä¸­çš„é»˜è®¤å€¼
        if hidden_dim is None:
            hidden_dim = MODEL_CONFIG['hidden_dim']
        if n_heads is None:
            n_heads = MODEL_CONFIG['n_heads']
        if n_layers is None:
            n_layers = MODEL_CONFIG['n_layers']
        if dropout is None:
            dropout = MODEL_CONFIG['dropout']
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=n_heads,
            batch_first=True,
            dropout=dropout
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # åˆ†ç±»å¤´
        self.cls_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1)
        )
    
    def forward(self, x, attention_mask=None):
        """
        Args:
            x: [batch_size, seq_len, input_dim]
            attention_mask: [batch_size, seq_len]
        """
        x = self.input_proj(x)
        
        if attention_mask is not None:
            src_key_padding_mask = (attention_mask == 0)
        else:
            src_key_padding_mask = None
        
        # Transformerç¼–ç 
        out = self.encoder(x, src_key_padding_mask=src_key_padding_mask)
        
        # å¹³å‡æ± åŒ–
        if attention_mask is not None:
            out_masked = out * attention_mask.unsqueeze(-1)
            valid_len = attention_mask.sum(dim=1, keepdim=True).clamp(min=1)
            pooled = out_masked.sum(dim=1) / valid_len
        else:
            pooled = out.mean(dim=1)
        
        # åˆ†ç±»
        logits = self.cls_head(pooled).squeeze(1)
        return logits


class FeatureAdapterModel(nn.Module):
    """
    ç‰¹å¾é€‚é…å™¨æ¨¡å‹ï¼šå°†105ç»´ç‰¹å¾æ˜ å°„åˆ°99ç»´ï¼Œç„¶åä¼ å…¥åŸå§‹æ¨¡å‹
    """
    
    def __init__(self, base_model: TransformerUserClassifier, input_dim: int = 105, target_dim: int = 99):
        super().__init__()
        self.base_model = base_model
        
        # ç‰¹å¾é™ç»´æ˜ å°„å±‚
        # ç®€å•çš„çº¿æ€§æ˜ å°„ï¼Œä¿ç•™æœ€é‡è¦çš„ç‰¹å¾
        self.feature_adapter = nn.Linear(input_dim, target_dim, bias=False)
        
        # åˆå§‹åŒ–é€‚é…å™¨æƒé‡ï¼šå‰99ç»´ç›´æ¥ä¼ é€’ï¼Œå6ç»´è¢«ä¸¢å¼ƒ
        with torch.no_grad():
            # åˆ›å»ºå•ä½çŸ©é˜µçš„å‰99åˆ—
            identity_mapping = torch.zeros(target_dim, input_dim)
            identity_mapping[:target_dim, :target_dim] = torch.eye(target_dim)
            self.feature_adapter.weight.data = identity_mapping
        
        logger.info(f"FeatureAdapter: {input_dim} -> {target_dim} dims")
    
    def forward(self, x, attention_mask=None):
        """
        Args:
            x: [batch_size, seq_len, 105] or [batch_size, 105]
            attention_mask: [batch_size, seq_len] or [batch_size]
        """
        # é€‚é…ç‰¹å¾ç»´åº¦
        if x.dim() == 3:  # [batch_size, seq_len, 105]
            batch_size, seq_len, _ = x.shape
            x_adapted = self.feature_adapter(x.view(-1, x.shape[-1])).view(batch_size, seq_len, -1)
        else:  # [batch_size, 105]
            x_adapted = self.feature_adapter(x)
        
        # ä¼ é€’ç»™åŸºç¡€æ¨¡å‹
        return self.base_model(x_adapted, attention_mask)


# ============================================================================
# é›†æˆç‰¹å¾æå–å™¨
# ============================================================================

class UserHistoryManager:
    """ç®¡ç†ç”¨æˆ·å†å²æ•°æ®ç”¨äºæ—¶åºç‰¹å¾è®¡ç®—"""
    
    def __init__(self):
        self.user_data = defaultdict(lambda: {
            'texts': [],
            'features': {
                'phq9_scores': [],
                'lsm_features': [],
                're_values': [],
                'cos_sim_values': [],
                'timestamps': []
            }
        })
    
    def add_text(self, user_id: str, text: str, timestamp: str = None):
        """æ·»åŠ æ–°æ–‡æœ¬åˆ°ç”¨æˆ·å†å²"""
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        self.user_data[user_id]['texts'].append(text)
        self.user_data[user_id]['features']['timestamps'].append(timestamp)
    
    def get_user_texts(self, user_id: str) -> List[str]:
        """è·å–ç”¨æˆ·æ‰€æœ‰å†å²æ–‡æœ¬"""
        return self.user_data[user_id]['texts']
    
    def add_feature_history(self, user_id: str, feature_type: str, value):
        """æ·»åŠ ç‰¹å¾å†å²"""
        if feature_type not in self.user_data[user_id]['features']:
            self.user_data[user_id]['features'][feature_type] = []
        self.user_data[user_id]['features'][feature_type].append(value)
    
    def get_feature_history(self, user_id: str, feature_type: str) -> List:
        """è·å–ç‰¹å¾å†å²"""
        return self.user_data[user_id]['features'].get(feature_type, [])
    
    def compute_time_series_stats(self, user_id: str, values: List[float], window_size: int = 5) -> Dict[str, float]:
        """è®¡ç®—æ—¶åºç»Ÿè®¡ç‰¹å¾"""
        if not values:
            return {
                'max_so_far': 0.0,
                'min_so_far': 0.0,
                'max_gap': 0.0,
                'delta': 0.0,
                'rolling_std': 0.0
            }
        
        values = np.array(values)
        max_so_far = float(np.max(values))
        min_so_far = float(np.min(values))
        max_gap = max_so_far - min_so_far
        delta = float(values[-1] - values[-2]) if len(values) > 1 else 0.0
        rolling_std = float(np.std(values[-window_size:])) if len(values) > 0 else 0.0
        
        return {
            'max_so_far': max_so_far,
            'min_so_far': min_so_far,
            'max_gap': max_gap,
            'delta': delta,
            'rolling_std': rolling_std
        }

class PHQ9Extractor:
    """PHQ-9 ç‰¹å¾æå–å™¨ï¼Œä½¿ç”¨æåŒ–åˆ†ææ–¹æ³•"""
    
    def __init__(self, device='cpu', model_name='mixedbread-ai/mxbai-embed-large-v1'):
        try:
            self.model = SentenceTransformer(model_name, device=device)
            self.device = device
            
            # PHQ-9 archetype scale items (ç®€åŒ–ç‰ˆ)
            self.scale_items = [
                "Little interest or pleasure in doing things",
                "Feeling down, depressed, or hopeless",
                "Trouble falling or staying asleep",
                "Feeling tired or having little energy",
                "Poor appetite or overeating",
                "Feeling bad about yourself",
                "Trouble concentrating on things",
                "Moving or speaking slowly or being fidgety",
                "Thoughts that you would be better off dead"
            ]
            
            # é¢„è®¡ç®—archetype embeddings
            self.scale_embeddings = self.model.encode(self.scale_items)
            logger.info(f"PHQ-9 extractor initialized with {len(self.scale_items)} scale items")
            
        except Exception as e:
            logger.warning(f"Failed to initialize PHQ-9 extractor: {e}")
            self.model = None
            self.scale_embeddings = None
    
    def extract(self, text: str, user_id: str, history_manager: UserHistoryManager) -> np.ndarray:
        """æå–PHQ-9ç‰¹å¾ (31ç»´)"""
        features = np.zeros(31, dtype=np.float32)
        
        if self.model is None or self.scale_embeddings is None:
            return features
        
        try:
            # è®¡ç®—æ–‡æœ¬embedding
            text_embedding = self.model.encode(text)
            
            # è®¡ç®—ä¸9ä¸ªarchetypeçš„ç›¸ä¼¼åº¦ (0-8)
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity([text_embedding], self.scale_embeddings)[0]
            features[0:9] = similarities.astype(np.float32)
            
            # è®¡ç®—æ€»ä½“PHQ-9åˆ†æ•°
            phq_score = float(np.mean(similarities))
            
            # æ·»åŠ åˆ°å†å²
            history_manager.add_feature_history(user_id, 'phq9_scores', phq_score)
            phq_history = history_manager.get_feature_history(user_id, 'phq9_scores')
            
            # æ—¶åºç»Ÿè®¡ç‰¹å¾ (9-15: 7ç»´)
            if phq_history:
                stats = history_manager.compute_time_series_stats(user_id, phq_history)
                features[9] = phq_score
                features[10] = stats['max_so_far']
                features[11] = stats['min_so_far']
                features[12] = stats['max_gap']
                features[13] = stats['delta']
                features[14] = stats['rolling_std']
                features[15] = len(phq_history)  # post_index
            
            # æ¯ä¸ªarchetypeçš„å†å²æœ€å¤§å€¼ (16-24: 9ç»´)
            for i in range(9):
                arch_history = [sim[i] for sim in [similarities] + [np.array(similarities)] * (len(phq_history) - 1)]
                if arch_history:
                    features[16 + i] = np.max(arch_history)
            
            # å…¶ä»–ç»Ÿè®¡ç‰¹å¾ (25-30: 6ç»´)
            if len(similarities) > 0:
                features[25] = np.mean(similarities)  # mean
                features[26] = np.max(similarities)   # max
                features[27] = np.min(similarities)   # min
                features[28] = np.std(similarities)   # std
                
            # è¶‹åŠ¿å’Œæ³¢åŠ¨æ€§
            if len(phq_history) > 1:
                features[29] = phq_history[-1] - phq_history[0]  # trend
                features[30] = np.std(phq_history)  # volatility
            
        except Exception as e:
            logger.warning(f"PHQ-9 extraction failed: {e}")
        
        return features

class CosineSimilarityExtractor:
    """ä½™å¼¦ç›¸ä¼¼åº¦ç‰¹å¾æå–å™¨"""
    
    def __init__(self, device='cpu', model_name='all-MiniLM-L6-v2'):
        try:
            self.model = SentenceTransformer(model_name, device=device)
            logger.info("Cosine similarity extractor initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize cosine similarity extractor: {e}")
            self.model = None
    
    def extract(self, text: str, previous_texts: List[str], user_id: str, history_manager: UserHistoryManager) -> np.ndarray:
        """æå–ä½™å¼¦ç›¸ä¼¼åº¦ç‰¹å¾ (6ç»´)"""
        features = np.zeros(6, dtype=np.float32)
        
        if self.model is None or not previous_texts:
            return features
        
        try:
            # è®¡ç®—ä¸æœ€è¿‘æ–‡æœ¬çš„ç›¸ä¼¼åº¦
            text_embedding = self.model.encode(text)
            prev_embedding = self.model.encode(previous_texts[-1])
            
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity([text_embedding], [prev_embedding])[0][0]
            
            features[0] = similarity
            
            # æ—¶åºç‰¹å¾
            history_manager.add_feature_history(user_id, 'cos_sim_values', similarity)
            sim_history = history_manager.get_feature_history(user_id, 'cos_sim_values')
            
            if sim_history:
                stats = history_manager.compute_time_series_stats(user_id, sim_history)
                features[1] = stats['max_so_far']
                features[2] = stats['min_so_far']
                features[3] = stats['max_gap']
                features[4] = stats['delta']
                features[5] = stats['rolling_std']
            
        except Exception as e:
            logger.warning(f"Cosine similarity extraction failed: {e}")
        
        return features

class LSMExtractor:
    """Language Style Matching (LSM) ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        # LIWC categories for LSM computation
        self.liwc_cols = ['ARTICLE', 'AUXVERB', 'CONJ', 'ADVERB', 'PREPS', 
                         'PRONOUN', 'QUANT', 'NEGATE', 'ASSENT', 'NONFLU']
        
        # Grouped categories for LSM dimensions
        self.social_cols = ['SOCIAL', 'FRIEND', 'FAMILY', 'HUMANS', 'AFFECT',
                           'POSEMO', 'POSFEEL', 'OPTIM', 'NEGEMO', 'ANX', 'ANGER', 'SAD', 'INHIB']
        self.positive_emo = ['POSEMO', 'POSFEEL', 'OPTIM']
        self.negative_emo = ['NEGEMO', 'ANX', 'ANGER', 'SAD']
        self.person = ['I', 'WE', 'SELF', 'YOU', 'SHEHE', 'OTHER']
        self.cognitive = ['INSIGHT', 'CAUSE', 'DISCREP', 'TENTAT', 'CERTAIN', 'INHIB']
        self.perceptual = ['SEE', 'HEAR', 'FEEL']
        
        logger.info("LSM extractor initialized")
    
    def extract(self, text: str, parent_text: str, user_id: str, history_manager: UserHistoryManager) -> np.ndarray:
        """æå–LSMç‰¹å¾ (60ç»´)"""
        features = np.zeros(60, dtype=np.float32)
        
        if not parent_text:
            return features
        
        try:
            # æå–LIWCç‰¹å¾
            body_liwc = extract_liwc(text)
            parent_liwc = extract_liwc(parent_text)
            
            # è®¡ç®—LSM for base categories
            EPS = 1e-6
            lsm_values = []
            
            for i, cat in enumerate(self.liwc_cols):
                body_val = body_liwc.get(cat, 0.0)
                parent_val = parent_liwc.get(cat, 0.0)
                lsm = 1 - abs(body_val - parent_val) / (body_val + parent_val + EPS)
                lsm_values.append(lsm)
                features[i] = lsm
            
            # LSM means for grouped categories
            def compute_group_lsm(categories):
                group_lsm = []
                for cat in categories:
                    if cat in liwc_keys:
                        body_val = body_liwc.get(cat, 0.0)
                        parent_val = parent_liwc.get(cat, 0.0)
                        lsm = 1 - abs(body_val - parent_val) / (body_val + parent_val + EPS)
                        group_lsm.append(lsm)
                return np.mean(group_lsm) if group_lsm else 0.0
            
            # Compute grouped LSM features
            features[10] = compute_group_lsm(self.social_cols)      # social_mean
            features[11] = compute_group_lsm(self.positive_emo)     # positive_emo_mean
            features[12] = compute_group_lsm(self.negative_emo)     # negative_emo_mean
            features[13] = compute_group_lsm(self.person)           # person_mean
            features[14] = compute_group_lsm(self.cognitive)        # cognitive_mean
            features[15] = compute_group_lsm(self.perceptual)       # perceptual_mean
            
            # Overall LSM statistics
            features[16] = np.mean(lsm_values) if lsm_values else 0.0  # LSM_mean
            features[17] = np.std(lsm_values) if lsm_values else 0.0   # LSM_std
            features[18] = np.max(lsm_values) if lsm_values else 0.0   # LSM_max
            features[19] = np.min(lsm_values) if lsm_values else 0.0   # LSM_min
            
            # Time series features (20-59: 40 dimensions)
            # Store LSM features in history
            current_lsm = features[0:20].copy()
            history_manager.add_feature_history(user_id, 'lsm_features', current_lsm)
            lsm_history = history_manager.get_feature_history(user_id, 'lsm_features')
            
            # Compute time-series stats for each LSM dimension
            if lsm_history and len(lsm_history) > 0:
                for dim in range(10):  # For first 10 LSM dimensions
                    dim_history = [feat[dim] for feat in lsm_history if len(feat) > dim]
                    if dim_history:
                        stats = history_manager.compute_time_series_stats(user_id, dim_history)
                        base_idx = 20 + dim * 4
                        features[base_idx] = stats['max_so_far']
                        features[base_idx + 1] = stats['min_so_far']
                        features[base_idx + 2] = stats['delta']
                        features[base_idx + 3] = stats['rolling_std']
            
        except Exception as e:
            logger.warning(f"LSM extraction failed: {e}")
        
        return features

class REExtractor:
    """Relative Entropy (RE) ç‰¹å¾æå–å™¨"""
    
    def __init__(self, device='cpu'):
        self.device = device
        try:
            # å°è¯•åŠ è½½é¢„è®­ç»ƒçš„æŠ‘éƒç—‡æ£€æµ‹æ¨¡å‹
            model_dir = "/u50/zhanh279/4Z03/jupyter/models"
            
            self.dep0_model = None
            self.dep1_model = None
            self.tokenizer = None
            
            # å°è¯•åŠ è½½æ¨¡å‹
            dep0_path = os.path.join(model_dir, "train_Dep0_LM")
            dep1_path = os.path.join(model_dir, "train_Dep1_LM")
            
            if os.path.exists(dep0_path) and os.path.exists(dep1_path):
                self.tokenizer = GPT2Tokenizer.from_pretrained(dep0_path)
                self.dep0_model = GPT2LMHeadModel.from_pretrained(dep0_path).to(device)
                self.dep1_model = GPT2LMHeadModel.from_pretrained(dep1_path).to(device)
                
                self.dep0_model.eval()
                self.dep1_model.eval()
                
                logger.info("RE extractor initialized with depression models")
            else:
                logger.warning("Depression models not found, RE features will be zeros")
                
        except Exception as e:
            logger.warning(f"Failed to initialize RE extractor: {e}")
            self.dep0_model = None
            self.dep1_model = None
            self.tokenizer = None
    
    def calc_loss(self, model, text: str) -> float:
        """è®¡ç®—æ¨¡å‹åœ¨æ–‡æœ¬ä¸Šçš„æŸå¤±"""
        if model is None or self.tokenizer is None:
            return 0.0
        
        try:
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs['input_ids'])
                loss = outputs.loss.item()
            
            return loss
        except Exception:
            return 0.0
    
    def extract(self, text: str, user_id: str, history_manager: UserHistoryManager) -> np.ndarray:
        """æå–REç‰¹å¾ (8ç»´)"""
        features = np.zeros(8, dtype=np.float32)
        
        try:
            # è®¡ç®—ä¸¤ä¸ªæ¨¡å‹çš„æŸå¤±
            loss_dep0 = self.calc_loss(self.dep0_model, text)
            loss_dep1 = self.calc_loss(self.dep1_model, text)
            
            features[0] = loss_dep0
            features[1] = loss_dep1
            
            # è®¡ç®—ç›¸å¯¹ç†µ
            re_value = loss_dep1 - loss_dep0
            features[2] = re_value
            
            # æ—¶åºç‰¹å¾
            history_manager.add_feature_history(user_id, 're_values', re_value)
            re_history = history_manager.get_feature_history(user_id, 're_values')
            
            if re_history:
                stats = history_manager.compute_time_series_stats(user_id, re_history)
                features[3] = stats['max_so_far']  # max_re_so_far
                features[4] = stats['min_so_far']  # min_re_so_far
                features[5] = stats['max_gap']     # max_gap_re
                features[6] = stats['delta']       # delta_re
                features[7] = stats['rolling_std'] # rolling_std_re
            
        except Exception as e:
            logger.warning(f"RE extraction failed: {e}")
        
        return features

class IntegratedFeatureExtractor:
    """é›†æˆç‰¹å¾æå–å™¨ - æå–æ‰€æœ‰105ç»´ç‰¹å¾"""
    
    def __init__(self, device='cpu'):
        self.device = device
        
        # åˆå§‹åŒ–å„ä¸ªç‰¹å¾æå–å™¨
        self.history_manager = UserHistoryManager()
        self.phq9_extractor = PHQ9Extractor(device)
        self.cos_sim_extractor = CosineSimilarityExtractor(device)
        self.lsm_extractor = LSMExtractor()
        self.re_extractor = REExtractor(device)
        
        logger.info("Integrated feature extractor initialized")
        
        # ç‰¹å¾ç»´åº¦éªŒè¯
        self.feature_dims = {
            'phq9': 31,
            'cos_sim': 6,
            'lsm': 60,
            're': 8
        }
        self.total_dims = sum(self.feature_dims.values())  # 105
        logger.info(f"Total feature dimensions: {self.total_dims}")
    
    def extract_from_eRisk_data(self, json_data: List[Dict], target_user_id: str) -> np.ndarray:
        """
        ä»eRisk APIæ•°æ®ä¸­æå–ç‰¹å¾
        
        Args:
            json_data: eRisk APIè¿”å›çš„JSONæ•°æ®
            target_user_id: ç›®æ ‡ç”¨æˆ·ID
        
        Returns:
            features: shape (n_texts, 105)
        """
        all_features = []
        user_texts = []
        
        for thread in json_data:
            if thread.get('targetSubject') != target_user_id:
                continue
            
            # å¤„ç†submission
            submission = thread.get('submission', {})
            if submission.get('author') == target_user_id:
                text = submission.get('body', '')
                if text.strip():
                    user_texts.append(text)
                    self.history_manager.add_text(target_user_id, text, submission.get('created_utc', ''))
            
            # å¤„ç†comments
            comments = thread.get('comments', [])
            for comment in comments:
                if comment.get('author') == target_user_id:
                    text = comment.get('body', '')
                    if text.strip():
                        # æ‰¾åˆ°parent textç”¨äºLSM
                        parent_text = ""
                        parent_id = comment.get('parent_id', '')
                        
                        # åœ¨submissionæˆ–å…¶ä»–commentsä¸­æ‰¾parent
                        if parent_id:
                            if submission.get('id') == parent_id:
                                parent_text = submission.get('body', '')
                            else:
                                for c in comments:
                                    if c.get('id') == parent_id:
                                        parent_text = c.get('body', '')
                                        break
                        
                        user_texts.append(text)
                        self.history_manager.add_text(target_user_id, text, comment.get('created_utc', ''))
                        
                        # å¦‚æœæ‰¾ä¸åˆ°parentï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ–‡æœ¬ä½œä¸ºparent
                        if not parent_text and len(user_texts) > 1:
                            parent_text = user_texts[-2]
                        
                        # æå–ç‰¹å¾
                        features = self.extract_features(text, target_user_id, parent_text, user_texts[:-1])
                        all_features.append(features)
        
        if not all_features:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ–‡æœ¬ï¼Œè¿”å›é›¶ç‰¹å¾
            return np.zeros((1, self.total_dims), dtype=np.float32)
        
        return np.array(all_features, dtype=np.float32)
    
    def extract_features(self, text: str, user_id: str, parent_text: str = "", previous_texts: List[str] = None) -> np.ndarray:
        """æå–å•ä¸ªæ–‡æœ¬çš„æ‰€æœ‰ç‰¹å¾"""
        if previous_texts is None:
            previous_texts = []
        
        features = np.zeros(self.total_dims, dtype=np.float32)
        
        try:
            # PHQ-9 features (0-30: 31 dims)
            phq9_features = self.phq9_extractor.extract(text, user_id, self.history_manager)
            features[0:31] = phq9_features
            
            # Cosine similarity features (31-36: 6 dims)
            cos_sim_features = self.cos_sim_extractor.extract(text, previous_texts, user_id, self.history_manager)
            features[31:37] = cos_sim_features
            
            # LSM features (37-96: 60 dims)
            lsm_features = self.lsm_extractor.extract(text, parent_text, user_id, self.history_manager)
            features[37:97] = lsm_features
            
            # RE features (97-104: 8 dims)
            re_features = self.re_extractor.extract(text, user_id, self.history_manager)
            features[97:105] = re_features
            
        except Exception as e:
            logger.error(f"Feature extraction failed for user {user_id}: {e}")
        
        return features

# ============================================================================
# æ›¿æ¢åŸæ¥çš„FeatureExtractorç±»
# ============================================================================

class FeatureExtractor:
    """
    é›†æˆçš„ç‰¹å¾æå–å™¨ï¼Œæä¾›ä¸åŸAPIå…¼å®¹çš„æ¥å£
    """
    
    def __init__(self, device='cpu'):
        self.integrated_extractor = IntegratedFeatureExtractor(device)
        self.feature_dim = 105  # æ€»ç‰¹å¾ç»´åº¦
        logger.info(f"FeatureExtractor initialized with {self.feature_dim} dimensions")
    
    def extract_from_texts(self, texts: List[str], user_id: str = "unknown_user") -> np.ndarray:
        """
        ä»æ–‡æœ¬åˆ—è¡¨æå–ç‰¹å¾ (å…¼å®¹æ€§æ–¹æ³•)
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            user_id: ç”¨æˆ·ID
        
        Returns:
            features: shape (num_messages, 105)
        """
        if not texts:
            return np.zeros((0, self.feature_dim), dtype=np.float32)
        
        all_features = []
        
        for i, text in enumerate(texts):
            previous_texts = texts[:i] if i > 0 else []
            parent_text = texts[i-1] if i > 0 else ""
            
            features = self.integrated_extractor.extract_features(text, user_id, parent_text, previous_texts)
            all_features.append(features)
        
        return np.array(all_features, dtype=np.float32)
    
    def extract_from_eRisk_data(self, json_data: List[Dict], target_user_id: str) -> np.ndarray:
        """ç›´æ¥å¤„ç†eRisk APIæ•°æ®"""
        return self.integrated_extractor.extract_from_eRisk_data(json_data, target_user_id)


# ============================================================================
# APIé€šä¿¡æ¨¡å—
# ============================================================================

class ERiskAPI:
    """
    ä¸eRiskæœåŠ¡å™¨é€šä¿¡
    """
    
    def __init__(self, team_token: str, base_url: str = BASE_URL):
        self.team_token = team_token
        self.base_url = base_url
        self.session = requests.Session()
    
    def get_discussions(self, retry_count=0) -> List[Dict]:
        """
        GETè¯·æ±‚è·å–è®¨è®ºï¼ˆç”¨æˆ·æ¶ˆæ¯ï¼‰
        
        Returns:
            è®¨è®ºåˆ—è¡¨ï¼Œæˆ–Noneè¡¨ç¤ºé”™è¯¯
        """
        url = f"{self.base_url}/getdiscussions/{self.team_token}"
        
        try:
            logger.info(f"[GET] è¯·æ±‚è®¨è®º: {url}")
            response = self.session.get(url, timeout=REQUEST_TIMEOUT)
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"âœ“ è·å¾— {len(data)} ä¸ªè®¨è®º")
                return data
            else:
                logger.error(f"âœ— GETå¤±è´¥: {response.status_code}")
                if retry_count < MAX_RETRIES:
                    time.sleep(RETRY_DELAY ** retry_count)
                    return self.get_discussions(retry_count + 1)
                return None
        
        except Exception as e:
            logger.error(f"âœ— ç½‘ç»œé”™è¯¯: {str(e)}")
            if retry_count < MAX_RETRIES:
                time.sleep(RETRY_DELAY ** retry_count)
                return self.get_discussions(retry_count + 1)
            return None
    
    def submit_decisions(self, run_id: int, decisions: List[Dict], retry_count=0) -> bool:
        """
        POSTè¯·æ±‚æäº¤å†³ç­–
        
        Args:
            run_id: è¿è¡Œç¼–å· (0-4)
            decisions: å†³ç­–åˆ—è¡¨
        
        Returns:
            æˆåŠŸä¸å¦
        """
        url = f"{self.base_url}/submit/{self.team_token}/{run_id}"
        
        try:
            logger.info(f"[POST] æäº¤Run {run_id}: {len(decisions)} ä¸ªç”¨æˆ·å†³ç­–")
            response = self.session.post(
                url,
                json=decisions,
                timeout=REQUEST_TIMEOUT
            )
            
            if response.status_code == 200:
                logger.info(f"âœ“ Run {run_id} æäº¤æˆåŠŸ")
                return True
            else:
                logger.error(f"âœ— POSTå¤±è´¥ (Run {run_id}): {response.status_code}")
                if retry_count < MAX_RETRIES:
                    time.sleep(RETRY_DELAY ** retry_count)
                    return self.submit_decisions(run_id, decisions, retry_count + 1)
                return False
        
        except Exception as e:
            logger.error(f"âœ— ç½‘ç»œé”™è¯¯ (Run {run_id}): {str(e)}")
            if retry_count < MAX_RETRIES:
                time.sleep(RETRY_DELAY ** retry_count)
                return self.submit_decisions(run_id, decisions, retry_count + 1)
            return False


# ============================================================================
# ä¸»å®¢æˆ·ç«¯ç±»
# ============================================================================

class ERiskClient:
    """
    eRisk T2 æµ‹è¯•å®¢æˆ·ç«¯
    """
    
    def __init__(self, model_path: str, team_token: str, num_runs: int = NUM_RUNS):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            team_token: å›¢é˜Ÿtoken
            num_runs: è¿è¡Œæ•°é‡ (1-5)
        """
        self.team_token = team_token
        self.num_runs = num_runs
        self.model_path = model_path
        
        # åˆå§‹åŒ–API
        self.api = ERiskAPI(team_token)
        
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # å…ˆåˆå§‹åŒ–ç‰¹å¾æå–å™¨ï¼ˆæ¨¡å‹åŠ è½½éœ€è¦çŸ¥é“ç‰¹å¾ç»´åº¦ï¼‰
        self.feature_extractor = FeatureExtractor(device=self.device)
        
        # ç„¶ååˆå§‹åŒ–æ¨¡å‹
        self.model = self._load_model()
        
        # ç”¨æˆ·æ•°æ®ç®¡ç†
        self.user_messages: Dict[str, List[str]] = {}  # ç”¨æˆ·çš„æ‰€æœ‰æ¶ˆæ¯
        self.user_alerts: Dict[str, int] = {}          # å·²è­¦æŠ¥ç”¨æˆ· + è­¦æŠ¥è½®æ•°
        self.user_scores: Dict[str, List[float]] = {}  # ç”¨æˆ·çš„å†å²è¯„åˆ†
        
        # ç»Ÿè®¡
        self.total_rounds = 0
        self.total_users = 0
        self.alerts_fired = 0
        
        logger.info(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ (Device: {self.device}, Runs: {num_runs})")
    
    def _load_model(self) -> TransformerUserClassifier:
        """åŠ è½½Transformeræ¨¡å‹"""
        try:
            logger.info(f"åŠ è½½æ¨¡å‹: {self.model_path}")
            
            if os.path.exists(self.model_path):
                # å…ˆåŠ è½½ç°æœ‰æ¨¡å‹æ£€æŸ¥ç»“æ„
                state_dict = torch.load(self.model_path, map_location=self.device, weights_only=False)
                
                # æ£€æŸ¥è¾“å…¥ç»´åº¦
                input_proj_weight = state_dict.get('input_proj.weight')
                if input_proj_weight is not None:
                    old_input_dim = input_proj_weight.shape[1]
                    logger.info(f"æ£€æµ‹åˆ°æ—§æ¨¡å‹è¾“å…¥ç»´åº¦: {old_input_dim}")
                    
                    if old_input_dim == 99 and self.feature_extractor.feature_dim == 105:
                        # éœ€è¦é€‚é…ï¼šä»105ç»´æ˜ å°„åˆ°99ç»´
                        logger.info("åˆ›å»º105->99ç»´ç‰¹å¾æ˜ å°„é€‚é…å™¨")
                        
                        # åˆ›å»º99ç»´æ¨¡å‹ï¼ˆä¸æ—§æ¨¡å‹å…¼å®¹ï¼‰
                        model = TransformerUserClassifier(
                            input_dim=99,
                            hidden_dim=MODEL_CONFIG['hidden_dim'],
                            n_heads=MODEL_CONFIG['n_heads'],
                            n_layers=MODEL_CONFIG['n_layers'],
                            dropout=MODEL_CONFIG['dropout']
                        )
                        
                        # ä¿®æ”¹åˆ†ç±»å¤´ç»“æ„ä»¥åŒ¹é…æ—§æ¨¡å‹
                        model.cls_head = nn.Sequential(
                            nn.Linear(MODEL_CONFIG['hidden_dim'], 64),
                            nn.ReLU(),
                            nn.Linear(64, 1)  # ç›´æ¥è¾“å‡ºï¼Œæ²¡æœ‰dropoutå±‚
                        )
                        
                        # åŠ è½½æ—§æ¨¡å‹æƒé‡
                        model.load_state_dict(state_dict)
                        logger.info("âœ“ æ—§æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
                        
                        # åŒ…è£…æ¨¡å‹ä»¥å¤„ç†105ç»´è¾“å…¥
                        model = FeatureAdapterModel(model, input_dim=105, target_dim=99)
                        logger.info("âœ“ ç‰¹å¾é€‚é…å™¨åˆ›å»ºæˆåŠŸ")
                        
                    else:
                        # æ­£å¸¸åŠ è½½
                        model = TransformerUserClassifier(
                            input_dim=old_input_dim,
                            hidden_dim=MODEL_CONFIG['hidden_dim'],
                            n_heads=MODEL_CONFIG['n_heads'],
                            n_layers=MODEL_CONFIG['n_layers'],
                            dropout=MODEL_CONFIG['dropout']
                        )
                        model.load_state_dict(state_dict)
                        logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
                else:
                    # åˆ›å»ºæ–°æ¨¡å‹
                    logger.warning("âš ï¸ æ— æ³•æ£€æµ‹æ¨¡å‹è¾“å…¥ç»´åº¦ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
                    model = TransformerUserClassifier(
                        input_dim=105,
                        hidden_dim=MODEL_CONFIG['hidden_dim'],
                        n_heads=MODEL_CONFIG['n_heads'],
                        n_layers=MODEL_CONFIG['n_layers'],
                        dropout=MODEL_CONFIG['dropout']
                    )
            else:
                logger.warning(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}ï¼Œä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹")
                model = TransformerUserClassifier(
                    input_dim=105,
                    hidden_dim=MODEL_CONFIG['hidden_dim'],
                    n_heads=MODEL_CONFIG['n_heads'],
                    n_layers=MODEL_CONFIG['n_layers'],
                    dropout=MODEL_CONFIG['dropout']
                )
            
            model.to(self.device)
            model.eval()
            
            return model
        
        except Exception as e:
            logger.error(f"âœ— åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            raise
    
    def _extract_user_texts(self, thread: Dict) -> List[str]:
        """
        ä»è®¨è®ºä¸­æå–ç›®æ ‡ç”¨æˆ·çš„æ‰€æœ‰æ¶ˆæ¯
        """
        target_user = thread.get('targetSubject')
        texts = []
        
        # æ£€æŸ¥æäº¤ä¸­çš„å†…å®¹
        submission = thread.get('submission', {})
        if submission.get('author') == target_user:
            body = submission.get('body', '')
            if body:
                texts.append(body)
        
        # æ£€æŸ¥è¯„è®ºä¸­çš„å†…å®¹
        comments = thread.get('comments', [])
        for comment in comments:
            if comment.get('author') == target_user:
                body = comment.get('body', '')
                if body:
                    texts.append(body)
        
        return texts
    
    def _predict_user_risk(self, user_id: str, discussions: List[Dict] = None) -> Tuple[float, float]:
        """
        é¢„æµ‹ç”¨æˆ·çš„é£é™©è¯„åˆ†
        
        Args:
            user_id: ç”¨æˆ·ID
            discussions: eRisk APIæ•°æ® (å¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨)
        
        Returns:
            (è¯„åˆ†0-1, å†³ç­–0/1)
        """
        try:
            if discussions is not None:
                # ä½¿ç”¨eRisk APIæ•°æ®ç›´æ¥æå–ç‰¹å¾
                features = self.feature_extractor.extract_from_eRisk_data(discussions, user_id)
                logger.debug(f"ä»APIæ•°æ®æå–ç‰¹å¾: ç”¨æˆ· {user_id}, shape: {features.shape}")
            else:
                # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä»ç”¨æˆ·æ¶ˆæ¯æå–ç‰¹å¾
                if user_id not in self.user_messages:
                    logger.warning(f"ç”¨æˆ· {user_id} æ— æ¶ˆæ¯è®°å½•")
                    return 0.5, 0
                
                texts = self.user_messages[user_id]
                features = self.feature_extractor.extract_from_texts(texts, user_id)
                logger.debug(f"ä»æ–‡æœ¬æå–ç‰¹å¾: ç”¨æˆ· {user_id}, shape: {features.shape}")
            
            if features.shape[0] == 0:
                logger.warning(f"ç”¨æˆ· {user_id} æ— æœ‰æ•ˆç‰¹å¾")
                return 0.5, 0
            
            # ä½¿ç”¨æœ€æ–°çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹
            latest_features = features[-1:] if len(features.shape) > 1 else features.reshape(1, -1)
            
            # è½¬ä¸ºå¼ é‡
            features_tensor = torch.from_numpy(latest_features).float().to(self.device)
            
            # ä¸ºå•ä¸ªæ ·æœ¬åºåˆ—åˆ›å»ºæ­£ç¡®çš„attention mask
            seq_len = 1  # æˆ‘ä»¬åªä½¿ç”¨æœ€æ–°çš„ä¸€ä¸ªç‰¹å¾å‘é‡
            mask = torch.ones(1, seq_len, dtype=torch.long).to(self.device)  # [batch_size, seq_len]
            
            # ç‰¹å¾éœ€è¦æ·»åŠ åºåˆ—ç»´åº¦: [batch_size, seq_len, feature_dim]
            if features_tensor.dim() == 2:  # [1, feature_dim]
                features_tensor = features_tensor.unsqueeze(1)  # [1, 1, feature_dim]
            
            # æ¨ç†
            with torch.no_grad():
                logits = self.model(features_tensor, mask)
                score = torch.sigmoid(logits).squeeze().item()
            
            # ä¿å­˜å†å²è¯„åˆ†
            if user_id not in self.user_scores:
                self.user_scores[user_id] = []
            self.user_scores[user_id].append(score)
            
            logger.debug(f"ç”¨æˆ· {user_id} è¯„åˆ†: {score:.4f}")
            
            return score, float(score)
        
        except Exception as e:
            logger.error(f"âœ— é¢„æµ‹å¤±è´¥ (ç”¨æˆ· {user_id}): {str(e)}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return 0.5, 0
    
    def _make_decision(self, user_id: str, score: float) -> int:
        """
        åŸºäºè¯„åˆ†åšå‡ºå†³ç­–
        
        Args:
            user_id: ç”¨æˆ·ID
            score: é£é™©è¯„åˆ†
        
        Returns:
            å†³ç­– (0æˆ–1)
        """
        # å¦‚æœå·²ç»è­¦æŠ¥è¿‡ï¼Œç»§ç»­è¿”å›0ï¼ˆæœ€ç»ˆå†³ç­–å·²åšï¼‰
        if user_id in self.user_alerts:
            return 0
        
        # åŸºäºé˜ˆå€¼åšå†³ç­–
        if score > DECISION_THRESHOLD:
            logger.info(f"âš ï¸ è­¦æŠ¥ï¼ç”¨æˆ· {user_id} é£é™©è¯„åˆ†: {score:.4f}")
            self.user_alerts[user_id] = self.total_rounds
            self.alerts_fired += 1
            return 1
        
        return 0
    
    def process_round(self) -> Tuple[List[Dict], bool]:
        """
        å¤„ç†ä¸€è½®è®¨è®º
        
        Returns:
            (å†³ç­–åˆ—è¡¨, æ˜¯å¦ç»§ç»­)
        """
        # è·å–è®¨è®º
        discussions = self.api.get_discussions()
        
        if discussions is None:
            logger.error("âœ— æ— æ³•è·å–è®¨è®ºï¼Œä¸­æ­¢")
            return [], False
        
        if not discussions:
            logger.info("âœ“ ç©ºåˆ—è¡¨ï¼Œæ‰€æœ‰æ•°æ®å·²å¤„ç†")
            return [], False
        
        self.total_rounds += 1
        logger.info(f"\n{'='*70}")
        logger.info(f"ç¬¬ {self.total_rounds} è½® - å¤„ç† {len(discussions)} ä¸ªè®¨è®º")
        logger.info(f"{'='*70}")
        
        # å¤„ç†æ¯ä¸ªè®¨è®º
        predictions = []
        
        for thread in discussions:
            target_user = thread.get('targetSubject')
            
            # ç»´æŠ¤ç”¨æˆ·æ¶ˆæ¯å†å²
            if target_user not in self.user_messages:
                self.user_messages[target_user] = []
                self.total_users += 1
            
            # æ·»åŠ æ–°æ¶ˆæ¯
            new_texts = self._extract_user_texts(thread)
            self.user_messages[target_user].extend(new_texts)
            
            logger.info(f"ç”¨æˆ· {target_user}: +{len(new_texts)} æ¡æ¶ˆæ¯ (æ€»è®¡: {len(self.user_messages[target_user])})")
            
            # é¢„æµ‹ (ç›´æ¥ä½¿ç”¨APIæ•°æ®è¿›è¡Œç‰¹å¾æå–)
            score, _ = self._predict_user_risk(target_user, discussions=[thread])
            
            # å†³ç­–
            decision = self._make_decision(target_user, score)
            
            # è®°å½•é¢„æµ‹
            predictions.append({
                'nick': target_user,
                'decision': decision,
                'score': score
            })
        
        logger.info(f"æœ¬è½®å¤„ç†å®Œæˆ: {len(predictions)} ç”¨æˆ·, {self.alerts_fired} ä¸ªè­¦æŠ¥")
        
        return predictions, True
    
    def run(self):
        """
        ä¸»å¾ªç¯ - æŒç»­å¤„ç†è½®æ•°ç›´åˆ°å®Œæˆ
        """
        logger.info(f"\nğŸš€ å¯åŠ¨eRisk T2å®¢æˆ·ç«¯")
        logger.info(f"Token: {self.team_token}")
        logger.info(f"Runs: {self.num_runs}")
        logger.info(f"Model: {self.model_path}")
        
        start_time = datetime.now()
        
        try:
            while True:
                # å¤„ç†ä¸€è½®
                predictions, continue_flag = self.process_round()
                
                if not continue_flag:
                    break
                
                # æäº¤æ‰€æœ‰runsçš„å†³ç­–
                all_submitted = True
                for run_id in range(self.num_runs):
                    success = self.api.submit_decisions(run_id, predictions)
                    if not success:
                        all_submitted = False
                        logger.error(f"âœ— Run {run_id} æäº¤å¤±è´¥")
                
                if not all_submitted:
                    logger.error("âœ— æŸäº›runsæäº¤å¤±è´¥ï¼Œä¸­æ­¢")
                    break
                
                # ç­‰å¾…ä¸‹ä¸€è½®
                logger.info("ç­‰å¾…ä¸‹ä¸€è½®...")
                time.sleep(2)
        
        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­")
        
        except Exception as e:
            logger.error(f"âœ— å¼‚å¸¸é”™è¯¯: {str(e)}")
        
        finally:
            # ç»Ÿè®¡ä¿¡æ¯
            elapsed_time = datetime.now() - start_time
            logger.info(f"\n{'='*70}")
            logger.info(f"æµ‹è¯•å®Œæˆ")
            logger.info(f"{'='*70}")
            logger.info(f"æ€»è½®æ•°: {self.total_rounds}")
            logger.info(f"æ€»ç”¨æˆ·: {self.total_users}")
            logger.info(f"è­¦æŠ¥æ•°: {self.alerts_fired}")
            logger.info(f"ç”¨æ—¶: {elapsed_time}")
            logger.info(f"å¹³å‡è½®å¤„ç†æ—¶é—´: {elapsed_time.total_seconds() / max(self.total_rounds, 1):.2f}s")


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """
    ä¸»ç¨‹åº
    """
    # ä½¿ç”¨config.pyä¸­çš„é…ç½®
    
    # æ£€æŸ¥token
    if TEAM_TOKEN == "YOUR_TEAM_TOKEN":
        logger.error("âŒ è¯·å…ˆåœ¨config.pyä¸­è®¾ç½®TEAM_TOKEN")
        return
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = ERiskClient(
        model_path=MODEL_PATH,
        team_token=TEAM_TOKEN,
        num_runs=NUM_RUNS
    )
    
    # è¿è¡Œ
    client.run()


if __name__ == "__main__":
    main()
